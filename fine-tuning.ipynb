{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and prepare the dataset\n",
    "data = pd.read_csv(\"updating_multiclass_dataset.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Display the first few rows to check the data structure\n",
    "print(data.head())\\\n",
    "\n",
    "\n",
    "# Map sentiments to integers (Adjust these mappings according to your dataset)\n",
    "sentiment_mapping = {'positive': 2, 'negative': 0, 'neutral': 1}\n",
    "data['Label'] = data['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Check for NaN values in the 'Label' column\n",
    "if data['Label'].isnull().any():\n",
    "    print(\"NaN values found in labels. Please check your mapping.\")\n",
    "    print(data[data['Label'].isnull()])  # Print rows with NaN labels\n",
    "else:\n",
    "    print(\"All labels are mapped correctly.\")\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text and include the labels\n",
    "    tokenized_output = tokenizer(examples['Text'], padding=\"max_length\", truncation=True)\n",
    "    tokenized_output['labels'] = examples['Label']  # Add labels to the tokenized output\n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Check if labels are correctly added\n",
    "print(\"Sample tokenized data:\", tokenized_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Split the dataset into training and validation sets\n",
    "train_testvalid = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_testvalid['train']\n",
    "small_train_dataset = train_dataset.shuffle(seed=42).select([i for i in list(range(1000))])  # Use 1000 samples\n",
    "valid_dataset = train_testvalid['test']\n",
    "small_valid_dataset = valid_dataset.shuffle(seed=42).select([i for i in list(range(200))])  # Use 200 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Load the BERT model for sequence classification\n",
    "num_labels = len(sentiment_mapping)  # Ensure num_labels corresponds to your label mapping\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"C:\\\\Users\\\\Ansh Srivastava\\\\Desktop\\\\BERT-model\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate only at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Try a higher batch size\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,  # Log every 100 steps for efficiency\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients to simulate a larger batch\n",
    "    fp16=True,  # Enable mixed precision training; disable if it slows down\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Define Trainer with model, arguments, and datasets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Start training\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(\"Training failed with error:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a directory to save the fine-tuned model and tokenizer\n",
    "save_directory = \"fine_tuned_model\"  # Replace with your desired save path\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer to check if they were saved correctly\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(save_directory)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Print a confirmation\n",
    "print(\"Model and tokenizer reloaded successfully from\", save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize the test dataset (assuming you have a 'test_dataset' DataFrame with 'Text' and 'Label' columns)\n",
    "test_dataset = Dataset.from_pandas(data)  # Replace `test_data` with your test DataFrame\n",
    "\n",
    "# Tokenize the test dataset\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 2: Evaluate the model\n",
    "metrics = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
    "print(\"Evaluation metrics:\", metrics)\n",
    "\n",
    "# Extracting accuracy specifically\n",
    "accuracy = metrics.get(\"eval_accuracy\", \"No accuracy metric found\")\n",
    "print(\"Model accuracy on test set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
